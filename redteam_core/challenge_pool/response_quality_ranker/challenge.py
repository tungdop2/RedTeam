from data_types import MinerInput, MinerOutput
from scipy.stats import spearmanr
import numpy as np
import os
import openai
import random
import json

SYSTEM_PROMT = """You are an AI assistant that generates multiple possible responses to question and ranks them by quality. For any given question, you will:
1. Generate multiple diverse responses that could answer the question
2. Rank these responses based on their completeness, accuracy, and helpfulness
3. Present them in this exact JSON format:
{
  "question": "The user's question",
  "response": ["Most comprehensive and helpful response", "Decent but incomplete response", "Basic or inadequate response"],
  "ranking": [1, 2, 3]
}

Guidelines for responses:
- Best responses (rank 1) can be:
    + Concise but complete
    + Detailed but relevant
    + Technical when appropriate
    + Simple but accurate
    + Include relevant examples or analogies

- Medium responses (rank 2) might have: 
    +  Minor factual errors
    +  Excessive or insufficient detail
    +  Unclear explanations
    +  Mixed accuracy
    +  Partial answers
    +  Overconfident tone with incomplete information
    
- Lower responses (rank 3+) should include various flaws:
    + Long, detailed responses with incorrect information
    + Confident tone but wrong conclusions
    + Circular reasoning or logical fallacies
    + Irrelevant information or tangents

Ensure that the degree of detail (length) of the responses is roughly similar.
Always generate at least 3 responses and ensure the ranking array matches the response array length."""

class Challenge:
    """
    A class that sets up the challenge and scores the miner's performance.
    It provides the task to be completed and evaluates the output.
    """
    def __init__(self):
        VLLM_URL = os.environ.get("VLLM_URL", "http://127.0.0.1:8000/v1") 
        VLLM_API_KEY = os.environ.get("API_KEY", "api-key")
        self.model_name = os.environ.get("VLLM_MODEL", "unsloth/Meta-Llama-3.1-8B-Instruct")
        self.client = openai.OpenAI(
            base_url=VLLM_URL,
            api_key=VLLM_API_KEY,
        )

        with open("questions.txt") as f:
            self.questions = f.readlines()

    def prepare_task(self, num_retries = 5) -> MinerInput:
        """
        Prepares the task by returning an instance of MinerInput,
        which contains the task description.
        """
        task = None
        while task is None and num_retries > 0:
            task = self._create_question_with_ranked_responses()
            num_retries -= 1
        prompt, responses, ranking = task["question"], task["response"], task["ranking"]

        combined = list(zip(responses, ranking))
        random.shuffle(combined)
        responses, ranking = zip(*combined)

        return MinerInput(prompt=prompt, responses=responses, groundtruth_ranking=ranking)

    def score_task(self, miner_input: MinerInput, miner_output: MinerOutput) -> float:
        """
        Evaluates the output generated by the miner.
        """
        score = self._compute_score(
            predictions=miner_output.response_quality,
            ground_truth=miner_input.groundtruth_ranking
        )
        return score["spearman_correlation"]

    def _create_question_with_ranked_responses(self):
        question = random.choice(self.questions)

        messages = [
            {"role": "system", "content": SYSTEM_PROMT},
            {"role": "user", "content": question}
        ]
        response = self._call_vllm(messages)


        # Try to parse the response as JSON
        try:
            parsed_response = json.loads(response)
            
            # Validate the required keys exist
            required_keys = ["question", "response", "ranking"]
            if not all(key in parsed_response for key in required_keys):
                print("Missing required keys in JSON response")
                return None
            
            # Validate response and ranking arrays have the same length
            if len(parsed_response["response"]) != len(parsed_response["ranking"]):
                print("Response and ranking arrays length mismatch: response=%d, ranking=%d", 
                           len(parsed_response["response"]), len(parsed_response["ranking"]))
                return None
            
            # Validate minimum number of responses
            if len(parsed_response["response"]) < 3:
                print("Insufficient number of responses: got %d, expected at least 3", len(parsed_response["response"]))
                return None
            return parsed_response
            
        except json.JSONDecodeError as e:
            print("Failed to parse JSON response: %s", str(e))
            return None
        
    
    def _call_vllm(self, messages):
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            max_tokens=1024,
        )
        print(response)
        content = response.choices[0].message.content
        return content

    def _compute_score(self, predictions, ground_truth):
        """
        Evaluate the model's ranking based on ground truth rankings.

        Args:
        - predictions (list): A list of model prediction results.
            Each element is a dictionary containing 'response_quality', higher is better.
        - ground_truth (list): A list of ground truth rankings (1 is the best).

        Returns:
        - dict: Evaluation results including Spearman's rank correlation and exact match status.
        """
        # Extract the model's ranking indices based on 'response_quality'
        model_scores = predictions
        model_ranking = (-np.array(model_scores)).argsort().argsort() + 1  # Sort in descending order, starting from rank 1

        # Compare the model's ranking with the ground truth ranking
        spearman_corr, _ = spearmanr(ground_truth, model_ranking)
        exact_match = ground_truth == list(model_ranking)
        
        return {
            "ground_truth": ground_truth,
            "model_ranking": list(model_ranking),
            "spearman_correlation": spearman_corr,
            "exact_match": exact_match,
        }
